{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ddae065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style> body {font-family: \"Roboto Condensed Light\", \"Roboto Condensed\";} h2 {padding: 10px 12px; background-color: #E64626; position: static; color: #ffffff; font-size: 40px;} .text_cell_render p { font-size: 15px; } .text_cell_render h1 { font-size: 30px; } h1 {padding: 10px 12px; background-color: #E64626; color: #ffffff; font-size: 40px;} .text_cell_render h3 { padding: 10px 12px; background-color: #0148A4; position: static; color: #ffffff; font-size: 20px;} h4:before{ \n",
       "    content: \"@\"; font-family:\"Wingdings\"; font-style:regular; margin-right: 4px;} .text_cell_render h4 {padding: 8px; font-family: \"Roboto Condensed Light\"; position: static; font-style: italic; background-color: #FFB800; color: #ffffff; font-size: 18px; text-align: center; border-radius: 5px;}input[type=submit] {background-color: #E64626; border: solid; border-color: #734036; color: white; padding: 8px 16px; text-decoration: none; margin: 4px 2px; cursor: pointer; border-radius: 20px;}</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA2001 Week 7 Tutorial\n",
    "# Material last updated: 7 Apr 2025\n",
    "# Note: this notebook was designed with the Roboto Condensed font, which can be installed here: https://www.1001fonts.com/roboto-condensed-font.html\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "    <style> body {font-family: \"Roboto Condensed Light\", \"Roboto Condensed\";} h2 {padding: 10px 12px; background-color: #E64626; position: static; color: #ffffff; font-size: 40px;} .text_cell_render p { font-size: 15px; } .text_cell_render h1 { font-size: 30px; } h1 {padding: 10px 12px; background-color: #E64626; color: #ffffff; font-size: 40px;} .text_cell_render h3 { padding: 10px 12px; background-color: #0148A4; position: static; color: #ffffff; font-size: 20px;} h4:before{ \n",
    "    content: \"@\"; font-family:\"Wingdings\"; font-style:regular; margin-right: 4px;} .text_cell_render h4 {padding: 8px; font-family: \"Roboto Condensed Light\"; position: static; font-style: italic; background-color: #FFB800; color: #ffffff; font-size: 18px; text-align: center; border-radius: 5px;}input[type=submit] {background-color: #E64626; border: solid; border-color: #734036; color: white; padding: 8px 16px; text-decoration: none; margin: 4px 2px; cursor: pointer; border-radius: 20px;}</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9424826",
   "metadata": {},
   "source": [
    "# Week 7 - Web APIs and Semi-Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a3032c7",
   "metadata": {},
   "source": [
    "This week will be going beyond scraping data from websites and using APIs to help collect data efficiently. Web APIs are purposefully provided by vendors to allow formal access to data, meaning it is often quite well-defined and consistent.\n",
    "\n",
    "This tutorial offers an introduction to the potential that APIs offer, and hopefully helps you consider the possibilities of data integrations in your future projects. It will also continue to focus on **semi-structured data**, and how we can transform this into a tabular form.\n",
    "\n",
    "Similarly to last week, our content will require the following Python libraries (none of which you should need to install):\n",
    "- **Requests** for interacting with websites and web services\n",
    "- **JSON** for handling JSON semi-structured objects\n",
    "- **Pandas** for dataframe management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2134f2",
   "metadata": {},
   "source": [
    "#### A note on the SQL Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eaa620",
   "metadata": {},
   "source": [
    "Much of this week's tutorial time will be spent conducting the **SQL Quiz assessment**. There won't be time to cover all the content below, but it is included for your reference so you can begin to apply hands-on examples of web APIs. The only \"tasks\" are towards the end and are SQL recap-based, rather than API tests. The accompanying text and descriptions, combined with the lecture, should be sufficient to read through and test out in your own time, and we will make solutions available as usual. Questions welcome on Ed, as always.\n",
    "\n",
    "The hope is that tutorial time will allow for **Sections 1** (for an introduction), **and 2** (for the benefit of the group assignment), but there is no issue if this needs to be left for next week to unpack in detail. Sections 3 and 4 are simply expansions of working with APIs and semi-structured data, and may not be covered in-person in either this week or the next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e656f2",
   "metadata": {},
   "source": [
    "## 1. Introduction to JSON and Web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca0d0a",
   "metadata": {},
   "source": [
    "Let's begin by exploring a few fun, simple examples of APIs with a variety of data types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d60f391",
   "metadata": {},
   "source": [
    "### 1.1 Exploring JSON with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33476cf5",
   "metadata": {},
   "source": [
    "At the core, it's worth acknowleding that accessing APIs is much like accessing a webpage, as we did in the Week 6 tutorial.\n",
    "\n",
    "Accordingly, we'll import the `requests` library to start, but also a little extra to allow later display of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84fcf8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import ipywidgets as widgets\n",
    "def display_image(response, w=200, h=300):\n",
    "    return widgets.Image(value=response.content, format='jpg', width=w, height=h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39331b",
   "metadata": {},
   "source": [
    "From there, let's test out the [Stanford Dogs Dataset](https://dog.ceo/dog-api/documentation/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921d6782",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://dog.ceo/api/breeds/list/all')\n",
    "response.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b011e",
   "metadata": {},
   "source": [
    "This output appears to be on the right track, but is definitely tricky to interpret in simple text format! It is really a `JSON` object - a common form of semi-structured data that may be reminiscent of our HTML encounters in Week 6. We can read it in appropriately using the ``.loads()`` function in the JSON library, which can parse a valid string into a JSON object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6691077f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "breeds = json.loads(response.text)\n",
    "breeds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220ebbc5",
   "metadata": {},
   "source": [
    "As seen by observing the keys above, or consulting the documentation, two things are returned - a \"message\", and the \"status\". We're interested in the \"message\". We can return output without further use of the JSON library, but it's worthwhile pointing out the ``.dumps()`` function, which can output JSON objects (or parts within) as simple strings, with desired formatting (e.g. indenting using `indent`, sorted keys using `sort_keys`, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baeda6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(breeds['message'], indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c29cff",
   "metadata": {},
   "source": [
    "The \"[breed](https://dog.ceo/dog-api/documentation/breed)\" page of the documentation defines the general format of a URL to return a random image of a selected dog breed. The function below leverages this to return a single image from the selected breed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c27da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_dog(breed):\n",
    "    # This command returns a URL to a random image of the selected dog breed.\n",
    "    # We would need to do another call of requests.get() to get the actual image.\n",
    "    response = requests.get(f'https://dog.ceo/api/breed/{breed}/images/random')\n",
    "    breed = json.loads(response.text)['message']\n",
    "    response_image = requests.get(breed)\n",
    "    return display_image(response_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b052b",
   "metadata": {},
   "source": [
    "Try choosing a breed from the JSON above, and returning an image for it below (currently returns a 'husky'):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f872c51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dog('husky')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685b2798",
   "metadata": {},
   "source": [
    "### 1.2 Exploring XML Objects with APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b737370e",
   "metadata": {},
   "source": [
    "We'll dig further into providing parameters to an API query in the next section, but we'll also quickly investigate other forms in which semi-structured data could be returned by an API. **XML** is another common format, that's more akin to HTML in its syntax.\n",
    "\n",
    "We'll try calling a specific API reference from the [World Bank API](https://documents.worldbank.org/en/publication/documents-reports/api), which provides many free economic metrics, among which population features. The URL below returns Australia's population over time, as represented in [their own webpages](https://data.worldbank.org/indicator/SP.POP.TOTL?locations=AU).\n",
    "\n",
    "The cell below will visually yield nothing of particular use, just the HTTP status code of our request (you should get [200](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes), which indicates success)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a42625",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\"https://api.worldbank.org/v2/country/AU/indicator/SP.POP.TOTL?date=1994:2024\")\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea657b7",
   "metadata": {},
   "source": [
    "Since XML is so similar to HTML, we can return to the BeautifulSoup library from last week, and use this to read in our data (this time using the `xml` parser, rather than `html5lib`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1db973",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "content = BeautifulSoup(response.text, 'xml')\n",
    "content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac57953",
   "metadata": {},
   "source": [
    "From there, we can again deconstruct the output in a similar manner. This is more to demonstrate what can be done - compare the code below to the XML output received when directly visiting the webpage requested yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc03988",
   "metadata": {},
   "outputs": [],
   "source": [
    "content = BeautifulSoup(response.text, 'lxml')  # parsing the webpage content as XML\n",
    "for x in content.find_all('wb:data'):  # iterating through each \"wb:data\" tag\n",
    "    if x.find('wb:value').text:  # if a value exists for a given row\n",
    "        print(x.find('wb:date').text, ': ', x.find('wb:value').text, sep='')  # print"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416fafa7",
   "metadata": {},
   "source": [
    "Ultimately, the above examples are simplistic, in that the URLs required to access data are self-contained (no further iteration necessary), and the information that is provided is singular (a simple dictionary or image). Often the data we seek to extract is much more complicated.\n",
    "\n",
    "We'll explore some further expansions in Sections 3-4, but first, with the group assignment pending, we'll segue into the extraction of geospatial data via APIs:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df866e6",
   "metadata": {},
   "source": [
    "## 2. Spatial Data APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af734b",
   "metadata": {},
   "source": [
    "APIs can return spatial data, which we'll cover in further depth next week (Week 8). The group assignment involves usage of the [NSW Points of Interest API](https://datasets.seed.nsw.gov.au/dataset/nsw-points-of-interest-poi), which we'll provide a demonstration of below.\n",
    "\n",
    "If you open up the \"SEED Map\" linked in the linked page above, you'll be able to scroll around and see many points of interest. Let's hone in on Fisher Library, for example. The cell below demonstrates how query string parameters can be used (noting of course that documentation must be consulted to know what fields and values are acceptable!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b69cd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'where': \"poiname='FISHER LIBRARY SYDNEY UNIVERSITY'\",\n",
    "    'outFields': '*',\n",
    "    'returnGeometry': 'true',\n",
    "    'f': 'json'\n",
    "}\n",
    "response = requests.get('https://maps.six.nsw.gov.au/arcgis/rest/services/public/NSW_POI/MapServer/0/query', params=params)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fc6582",
   "metadata": {},
   "source": [
    "Hopefully the above cell returns a HTTP status code of 200, to indicate success. Unpacking the actual data itself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e4bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "places = json.loads(response.text)\n",
    "print(json.dumps(places['features'][0], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c81bd4",
   "metadata": {},
   "source": [
    "The above example is helpful if we want to return information about a specific POI already known by name to us. Instead of our `where` parameter, we'll utilise the API's `geometry` field, which allows us to define a geographical area of region of interest (by defining the corners via `xmin`, `xmax`, `ymin` and `ymax`).\n",
    "\n",
    "Consider the below helper function, which allows users to define a midpoint coordinate, then returns nearby POI values. The `boxsize` argument can be adjusted for wider or finer margins, and is defined in kilometres (e.g. `boxsize=5` as the default will yield a 5km by 5km region of interest around the midpoint):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f22a213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearbyPOI(coordinates, boxsize=5, filters={}):\n",
    "    baseURL = 'https://maps.six.nsw.gov.au/arcgis/rest/services/public/NSW_POI/MapServer/0/query'\n",
    "    lat, lon = round(coordinates[0], 5), round(coordinates[1], 5)\n",
    "    delta = boxsize/(100*2)  # Australia's DCCEEW defines 1 of a degree of latitude as roughly 1/100th of a km, hence division by 100. The extra division by 2 is to ensure the full width is divided in each direction (e.g. 2.5km left and right)\n",
    "    params = {\n",
    "        'geometry': f'\"xmin\":{lon-delta},\"ymin\":{lat-delta},\"xmax\":{lon+delta},\"ymax:{lat+delta}\"',\n",
    "        'outFields': '*',\n",
    "        'returnGeometry': 'true',\n",
    "        'f': 'json'\n",
    "    }\n",
    "    response = requests.get(baseURL, params)\n",
    "    return json.loads(response.text)['features']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04cb253",
   "metadata": {},
   "source": [
    "According to Google, the GPS coordinates of the USYD Quadrangle are as defined below. Let's test out our function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d6ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinates = (-33.88566318983135, 151.18885144061616) \n",
    "results = nearbyPOI(coordinates, boxsize=1)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a4c6b3",
   "metadata": {},
   "source": [
    "There's a lot there! Summarising further to sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1e2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[x['attributes']['poiname'] for x in results if x['attributes']['poiname']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f91b57",
   "metadata": {},
   "source": [
    "#### A parting note on application for the group assignment..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c526a5",
   "metadata": {},
   "source": [
    "The above function we built returns anything nearby a given **point** (latitude/longitude coordinates of a midpoint), *using* a bounding box.\n",
    "\n",
    "This will need to be extended for the group assignment, as the requirements there are to iterate through each SA2 area within your selected regions, to find its bounding box, and to feed **this bounding box** into a similar function (i.e. not midpoint based).\n",
    "\n",
    "Next week we'll also look further into how the geographic output (i.e. the x/y coordinates of each returned POI) are best to be processed and stored, both in Pandas (GeoPandas) and SQL (PostGIS). More on that then - but the example above should suffice for demonstration purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b3c04b",
   "metadata": {},
   "source": [
    "## 3. Extracting Data Using Web APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e7f14",
   "metadata": {},
   "source": [
    "Most of the following examples will focus on [Project Gutenberg](https://en.wikipedia.org/wiki/Project_Gutenberg), an altriustic undertaking that has focused on digitising and providing culturally important texts for now 50 years. As part of this motivation to ensure open access to all, there is a \"[Gutendex](https://gutendex.com/)\" which allows API access to metadata of all books in the collection. Check out it's documentation (linked above)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f606e6",
   "metadata": {},
   "source": [
    "### 3.1 Nested JSON Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637931dc",
   "metadata": {},
   "source": [
    "As described in the documentation, we can load in book data from across their library by combining the URL they provide, and our requests/JSON Python approach from before. This returns a JSON objects with a few attributes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d314ace3",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = json.loads(requests.get('https://gutendex.com/books').text)\n",
    "books.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1880b776",
   "metadata": {},
   "source": [
    "If we briefly investigate the data types of each of the four objects, we can see `count` is a number, `next` is a simple string, `previous` is empty, and `results` will likely contain the bulk of what we're interested in, given it is a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf18d67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in books.keys():\n",
    "    print(k, type(books[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ee3dd7",
   "metadata": {},
   "source": [
    "Equipped with this knowledge, we can investigate the values of each. The `count` reveals the current size of the Gutenberg digital library - **over 75,000 texts**! Yet despite so many texts, it appears **only 32 have been returned** in our `results` object. This is intentional, to avoid our API call unintentionally extracting everything at once, which could be a massive download of information (even when just metadata!). There is a way around this involving pagination, which we'll discuss later, and involves the `next` field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7bbd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Count:', books['count'])\n",
    "print('Results:', len(books['results']))\n",
    "print('Next:', books['next'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be451554",
   "metadata": {},
   "source": [
    "Let's investigate our `results` object by considering just an arbitrary item - Shakespeare's famous play \"Romeo and Juliet\" is the fifth most downloaded (index #4), for example.\n",
    "\n",
    "This is a good example of a proper **JSON object**, which is effectively just a combination of nested lists and dictionaries. For example, while `id` may be a simple field with a single number, `authors` is a list, in case a text was written by multiple people, and each value within the list is a dictionary, to allow distinction between author fields (their name, birth year and death year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c460333",
   "metadata": {},
   "outputs": [],
   "source": [
    "books['results'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6068881",
   "metadata": {},
   "source": [
    "### 3.2 Requesting Links within Objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee242c2",
   "metadata": {},
   "source": [
    "In our Romeo and Juliet example above, notice within the final key `formats`, there exists an `image/jpeg` key which contains a URL. If we navigate to that field and request that link from the web, we can render its logo using our image display skills from earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b86d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(requests.get(books['results'][4]['formats']['image/jpeg']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555f8d0",
   "metadata": {},
   "source": [
    "To be super comprehensive, we could even notice a `text/html` link is also provided. Using our **web scraping** skills from last week's tutorial, we could additionally fetch this URL, and parse its webpage contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4215ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_source = requests.get(books['results'][4]['formats']['text/html']).text\n",
    "content = BeautifulSoup(webpage_source, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e22d978",
   "metadata": {},
   "source": [
    "From there, for example, we could print all `h2` and `h3` fields within it, which provides a nice simple overview of what to expect within the classic play (code below also adds in a line-break after the major headings, for ease of reading):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee0119b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for header in content.find_all(['h2', 'h3']):\n",
    "    if header.name == 'h2':\n",
    "        print('_____\\n')\n",
    "    print(header.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c5d17",
   "metadata": {},
   "source": [
    "But we won't delve too deeply into that yet - a whole week on text data awaits after the break :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614485e7",
   "metadata": {},
   "source": [
    "### 3.3 Query Parameters and Pagination"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db74a862",
   "metadata": {},
   "source": [
    "Commonly within APIs, further **query parameters** can be provided to the website when requesting its content. These are commonly achieved by a `?` at the end of the URL, followed by a simple key-value list of all conditions required.\n",
    "\n",
    "_(with the `requests` library in Python, we can also pass them as parameters via a dictionary, so this has also been included as a code comment)_\n",
    "\n",
    "For example in the [Gutendex docs](https://gutendex.com/), a \"topic\" can be selected of the user's choice, for the data it returns. A 20th century American author Anne Haight compiled a collection of \"[banned books](https://www.gutenberg.org/ebooks/bookshelf/336)\" from different points in time around the world, which we can access by restricting our API call to only those with a \"topic\" of 'banned'. This will return a much smaller sample of less than 200 texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1e8976",
   "metadata": {},
   "outputs": [],
   "source": [
    "bannedbooks = json.loads(requests.get('https://gutendex.com/books?topic=banned').text)\n",
    "#bannedbooks = json.loads(requests.get('https://gutendex.com/books', params={'topic': 'banned'}).text)\n",
    "print('Count:', bannedbooks['count'])\n",
    "print('Results:', len(bannedbooks['results']))\n",
    "print('Next:', bannedbooks['next'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7ed2d8",
   "metadata": {},
   "source": [
    "It's now worth noting the point of the `next` field. Since Gutendex API calls are limited to 32 results at a time, each call will indicate how to retrieve the *next* 32 rows on the next \"page\". For our banned books, this is a very similar URL, just now also with a `page=2` query parameter.\n",
    "\n",
    "By providing this, we don't have to try and reverse-engineer the URLs to return a larger dataset, we can simply iterate through, each time noting the `next` link, until one no longer exists. The query below does just that, with attached code comments so you can follow along:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6744bf75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t  # using the inbuilt \"time\" module for explicit wait times in our code\n",
    "\n",
    "keepgoing = True  # establishing a simple variable indicating if the looping should continue\n",
    "URL = 'https://gutendex.com/books?topic=banned'  # our base URL to begin with\n",
    "results = []  # an empty list ready to store our results as we go\n",
    "\n",
    "print('Data loading begins...')  # message to the user\n",
    "while keepgoing:  # a WHILE loop that depends on the \"keepgoing\" variable being True\n",
    "    t.sleep(2)  # purposefully waiting a minute before we retrieve the URL's contents\n",
    "    print(URL)  # printing the link we're using in the API call\n",
    "    page = json.loads(requests.get(URL).text)  # retrieving the content as a JSON object\n",
    "    results += page['results']  # adding the 'results' section to our stored list\n",
    "    if page['next']:  # if there is a 'next' field of results noted\n",
    "        URL = page['next']  # then make this our new URL for when the loop repeats\n",
    "    else:  # if there is no longer a 'next' field\n",
    "        keepgoing = False  # then we have reached the end and can terminate the loop\n",
    "        print('Data load complete.')  # message to the user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f194a9",
   "metadata": {},
   "source": [
    "Despite being a semi-structured datasource, we can still process our results as a Pandas dataframe, if only to test it has worked correctly. It will be a little messy, given some fields are lists or lists of dictionaries, but we can see the 181 texts below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75310ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "bannedbooks = pd.DataFrame(results)\n",
    "bannedbooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290046e1",
   "metadata": {},
   "source": [
    "## 4. Transforming Semi-Structured Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b0d2d2",
   "metadata": {},
   "source": [
    "If we are to answer meaningful questions on our dataset, our best case scenario would be achieving a structured representation of the data model, which we can query."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6886596e",
   "metadata": {},
   "source": [
    "### 4.1 Spinning Off Entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a94a3c",
   "metadata": {},
   "source": [
    "For some columns, this is easy. The following columns are simple values from our main dataframe without further nested depth, so we'll store these as our `booksdf` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015cbcd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "booksdf = bannedbooks[['id', 'title', 'copyright', 'media_type', 'download_count']]\n",
    "booksdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011478b",
   "metadata": {},
   "source": [
    "Other fields, however, are more complex, such as \"subjects\". As a simple list, this would be best spun out into one row for each subject value. We can achieve this using Pandas' dramatically-named `explode()` function, and store this as another dataframe for now - `subjectsdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9f3ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subjectsdf = bannedbooks[['id', 'subjects']]\n",
    "subjectsdf = subjectsdf.explode('subjects')\n",
    "subjectsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6f77ab",
   "metadata": {},
   "source": [
    "### 4.2 Complex Entity Transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f1d36c",
   "metadata": {},
   "source": [
    "Fields such as \"authors\" are even more complex again. Recall each value here is a dictionary, which contains a list of authors (potentially more than one), so we can begin by spinning this off into one row per author of each book, like the \"subjects\" approach above. This increases our row count from 181 to 187, so there are a handful of books with multiple attributed authors. We'll store this as `authorsdf`, though each row still contains a dictionary of information for each author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c4b070f",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorsdf = bannedbooks[['id', 'authors']]\n",
    "authorsdf = authorsdf[['id', 'authors']].explode('authors').reset_index(drop=True)\n",
    "authorsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb5859e",
   "metadata": {},
   "source": [
    "It's worth pointing out that the Pandas transformations shown below are **purely for our benefit, and not examinable functions you need to remember**. Don't stress about the next couple of code blocks - they are not super important but explained for transparency.\n",
    "\n",
    "The \"authors\" column really should be spun out into three separate columns, one for each attribute (birth_year, death_year, name). Pandas considers the data type of this column to be a Series, which is necessary knowledge to enable the split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0097bc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(authorsdf.authors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e00c92",
   "metadata": {},
   "source": [
    "Using the `apply` function, we can split it out into one column for each attribute, and then join it back in-place of the original column, to produce a much more friendly output - our final version of `authorsdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aadd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "authorfields = authorsdf['authors'].apply(pd.Series).drop(0, axis=1)\n",
    "authorsdf = authorsdf.join(authorfields).drop('authors', axis=1).reset_index(drop=True)\n",
    "authorsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08cedb4",
   "metadata": {},
   "source": [
    "One final entity we'll spin off - the \"formats\". Code is condensed below since not important, but to summarise, it acts similarly to \"authors\", but doesn't need to expect a list of dictionaries, simply a single dictionary (hence no `.explode()` function). It does, however, require one extra transformation using `.melt()` so that we don't have one column per format, but rather a simple key-value table.\n",
    "\n",
    "Again - not super important, just for those interested in the magic behind the scenes! What's important is that now we have **four, much more friendly, structured tables** for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df3b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "formatsdf = bannedbooks[['id', 'formats']]\n",
    "formatfields = formatsdf['formats'].apply(pd.Series)\n",
    "formatsdf = formatsdf.join(formatfields).drop('formats', axis=1).reset_index(drop=True)\n",
    "formatsdf = formatsdf.melt(id_vars=['id']).dropna(subset=['value'])\n",
    "formatsdf.rename(columns={'variable': 'format', 'value': 'link'}, inplace=True)\n",
    "formatsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a518d3ab",
   "metadata": {},
   "source": [
    "Summarising the dimensions of our four new cleaned tables for a sense of satisfaction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bf552b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Books:', booksdf.shape)\n",
    "print('Subjects:', subjectsdf.shape)\n",
    "print('Authors:', authorsdf.shape)\n",
    "print('Formats:', formatsdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3ba671",
   "metadata": {},
   "source": [
    "### 4.3 Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdae058",
   "metadata": {},
   "source": [
    "The below functions should be quite familiar by now - since we've done the hard yards, let's again import it into our localhost database and run a few queries to see if we can manage some interesting findings.\n",
    "\n",
    "Recall also this depends on your individual `Credentials.json` from previous weeks existing in your directory again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26198c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "credentials = \"Credentials.json\"\n",
    "\n",
    "def pgconnect(credential_filepath, db_schema=\"public\"):\n",
    "    with open(credential_filepath) as f:\n",
    "        db_conn_dict = json.load(f)\n",
    "        host       = db_conn_dict['host']\n",
    "        db_user    = db_conn_dict['user']\n",
    "        db_pw      = db_conn_dict['password']\n",
    "        default_db = db_conn_dict['user']\n",
    "        try:\n",
    "            db = create_engine('postgresql+psycopg2://'+db_user+':'+db_pw+'@'+host+'/'+default_db, echo=False)\n",
    "            conn = db.connect()\n",
    "            print('Connected successfully.')\n",
    "        except Exception as e:\n",
    "            print(\"Unable to connect to the database.\")\n",
    "            print(e)\n",
    "            db, conn = None, None\n",
    "        return db,conn\n",
    "\n",
    "def query(conn, sqlcmd, args=None, df=True):\n",
    "    result = pd.DataFrame() if df else None\n",
    "    try:\n",
    "        if df:\n",
    "            result = pd.read_sql_query(sqlcmd, conn, params=args)\n",
    "        else:\n",
    "            result = conn.execute(text(sqlcmd), args).fetchall()\n",
    "            result = result[0] if len(result) == 1 else result\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered: \", e, sep='\\n')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc177f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "db, conn = pgconnect(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d5d3f",
   "metadata": {},
   "source": [
    "Let's create a new schema for our data, simply entitled \"Books\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab159d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(text(\"create schema if not exists Books\"))\n",
    "conn.execute(text(\"set search_path to Books\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18948f3d",
   "metadata": {},
   "source": [
    "From there, we'll create tables for our three spun off datasets, and populate them from Pandas as below. This process should be quite familiar to you by now - we'll check it worked by selecting all from \"Authors\" at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202cca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(text(\"\"\"\n",
    "DROP TABLE IF EXISTS Subjects;\n",
    "DROP TABLE IF EXISTS Authors;\n",
    "DROP TABLE IF EXISTS Formats;\n",
    "\n",
    "CREATE TABLE Subjects(\n",
    "   id int,\n",
    "   subjects varchar(1000)\n",
    ");\n",
    "CREATE TABLE Authors(\n",
    "   id int,\n",
    "   birth_year int,\n",
    "   death_year int,\n",
    "   name varchar(100)\n",
    ");\n",
    "CREATE TABLE Formats(\n",
    "   id int,\n",
    "   format varchar(100),\n",
    "   link text\n",
    ");\n",
    "\"\"\"))\n",
    "subjectsdf.to_sql(\"subjects\", con=conn, if_exists='append', index=False)\n",
    "authorsdf.to_sql(\"authors\", con=conn, if_exists='append', index=False)\n",
    "formatsdf.to_sql(\"formats\", con=conn, if_exists='append', index=False)\n",
    "query(conn, \"select * from Authors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f56828",
   "metadata": {},
   "source": [
    "**Task: Create a table for Books and populate it.**\n",
    "\n",
    "Using our `booksdf` from above, this is the only one of our four that now isn't populated in our localhost database. Similarly to the examples above, define a \"Books\" table using a `CREATE TABLE` command, then populate it with our Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e4f576",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "conn.execute(text(\"\"\"\n",
    "\n",
    ");\"\"\"))\n",
    "booksdf.to_sql(\"books\", con=conn, if_exists='append', index=False)\n",
    "query(conn, \"select * from Books\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb411852",
   "metadata": {},
   "source": [
    "### 4.4 Data Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5a8c2a",
   "metadata": {},
   "source": [
    "**Task: Attempt the four SQL queries below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8059889",
   "metadata": {},
   "source": [
    "**a) Find all books attributed to authors *without a birth year*, sorted in alphabetical order.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126c977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "query(conn, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24446910",
   "metadata": {},
   "source": [
    "**b) Find the top 5 authors with the most number of banned books.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808ddbd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "query(conn, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a0474",
   "metadata": {},
   "source": [
    "**c) Produce a list of all 'audio' formats available for texts in our dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4204c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "query(conn, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84be4d52",
   "metadata": {},
   "source": [
    "**d) Which subject containing at least 5 banned books has the highest average download count?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4a6f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "query(conn, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175fd66c",
   "metadata": {},
   "source": [
    "#### Challenge: Feel free to investigate other account-based APIs!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df972ff2",
   "metadata": {},
   "source": [
    "That concludes this week's content - but do consider the next step of APIs, which is those that require authentication to access data. Most platforms, big and small, are structured this way (locally, examples like the NSW Government, and globally, tech giants like YouTube and OpenAI). Feel free to explore further with examples like these!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
