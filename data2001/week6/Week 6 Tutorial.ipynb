{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ddae065",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <style> body {font-family: \"Roboto Condensed Light\", \"Roboto Condensed\";} h2 {padding: 10px 12px; background-color: #E64626; position: static; color: #ffffff; font-size: 40px;} .text_cell_render p { font-size: 15px; } .text_cell_render h1 { font-size: 30px; } h1 {padding: 10px 12px; background-color: #E64626; color: #ffffff; font-size: 40px;} .text_cell_render h3 { padding: 10px 12px; background-color: #0148A4; position: static; color: #ffffff; font-size: 20px;} h4:before{ \n",
       "    content: \"@\"; font-family:\"Wingdings\"; font-style:regular; margin-right: 4px;} .text_cell_render h4 {padding: 8px; font-family: \"Roboto Condensed Light\"; position: static; font-style: italic; background-color: #FFB800; color: #ffffff; font-size: 18px; text-align: center; border-radius: 5px;}input[type=submit] {background-color: #E64626; border: solid; border-color: #734036; color: white; padding: 8px 16px; text-decoration: none; margin: 4px 2px; cursor: pointer; border-radius: 20px;}</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA2001 Week 6 Tutorial\n",
    "# Material last updated: 31 Mar 2025\n",
    "# Note: this notebook was designed with the Roboto Condensed font, which can be installed here: https://www.1001fonts.com/roboto-condensed-font.html\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML('''\n",
    "    <style> body {font-family: \"Roboto Condensed Light\", \"Roboto Condensed\";} h2 {padding: 10px 12px; background-color: #E64626; position: static; color: #ffffff; font-size: 40px;} .text_cell_render p { font-size: 15px; } .text_cell_render h1 { font-size: 30px; } h1 {padding: 10px 12px; background-color: #E64626; color: #ffffff; font-size: 40px;} .text_cell_render h3 { padding: 10px 12px; background-color: #0148A4; position: static; color: #ffffff; font-size: 20px;} h4:before{ \n",
    "    content: \"@\"; font-family:\"Wingdings\"; font-style:regular; margin-right: 4px;} .text_cell_render h4 {padding: 8px; font-family: \"Roboto Condensed Light\"; position: static; font-style: italic; background-color: #FFB800; color: #ffffff; font-size: 18px; text-align: center; border-radius: 5px;}input[type=submit] {background-color: #E64626; border: solid; border-color: #734036; color: white; padding: 8px 16px; text-decoration: none; margin: 4px 2px; cursor: pointer; border-radius: 20px;}</style>\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9424826",
   "metadata": {},
   "source": [
    "# Week 6 - Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6c63d",
   "metadata": {},
   "source": [
    "Not all data is presented as neatly as a structured dataframe of rows and columns, like we've become accustomed to in the previous weeks. Often, meaningful information exists in unstructured or semi-structured formats. Take for example, the internet. Worlds of information exist across millions of webpages, and extracting fields of interest from these pages is our focus today.\n",
    "\n",
    "This will require the following Python libraries:\n",
    "- **Request**         for interacting with websites and web services\n",
    "- **BeautifulSoup**   for webpage parsing\n",
    "- **HTML5Lib**        for the actual parser that BeautifulSoup uses\n",
    "- **Pandas**          for dataframe management\n",
    "\n",
    "To use the above, you will need to have the following libraries installed (using either pip3 or Anaconda navigator):\n",
    "- `bs4`\n",
    "- `html5lib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be18c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e656f2",
   "metadata": {},
   "source": [
    "## 1. Scraping Data from a Webpage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ca0d0a",
   "metadata": {},
   "source": [
    "We'll start with a familiar example, and read in the webpage for [this unit's outline](https://www.sydney.edu.au/units/DATA2001/2025-S1C-ND-CC) on the USYD website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4deac11",
   "metadata": {},
   "source": [
    "### 1.1 Webpage Retrieval and Parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8512b902",
   "metadata": {},
   "source": [
    "The `requests` library can be used to `get()` the contents of a page, as seen below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f96ed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "webpage_source = requests.get(\"https://www.sydney.edu.au/units/DATA2001/2025-S1C-ND-CC\").text\n",
    "print(webpage_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b7918b",
   "metadata": {},
   "source": [
    "This output of this request is the raw webpage source code. This is normally parsed and rendered by a web browser as a nice visual webpage.\n",
    "\n",
    "The language in which this webpage is written, is called **HTML** (the *Hypertext Markup Language*), and is a tree-like structure of content elements. We can interpret this content using a **HTML parser** - several exist, but we'll be using *BeautifulSoup*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41a8a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "content = BeautifulSoup(webpage_source, 'html5lib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab7b5b8",
   "metadata": {},
   "source": [
    "### 1.2 Traversing the Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e293a597",
   "metadata": {},
   "source": [
    "The key benefit of parsing the webpage is that we can now **locate and iterate through the HTML content** by either traversing the tree, or by selecting particular HTML tags, classes or identifiers. As a simple example, the webpage output contains a single instance of a `title` tag, as seen below:\n",
    "\n",
    "`<title>Outline - The University of Sydney</title>`\n",
    "\n",
    "This title is typically reflected in the browser tab name, which you'll notice here doesn't align with our code. In the web browser's code, the title reflects the unit of study name - \"DATA2001: Semester 1, 2025\". When first loaded though, there's a brief second where the tab name is \"Outline - The University of Sydney\", like our code here. For more complex web pages, some content may be dynamically generated on load (e.g. with Javascript), and hence small disparities like these can occur. This is the only field we'll encounter in this tutorial's example that should differ.\n",
    "\n",
    "Nonetheless, using BeautifulSoup, we can extract this information, by finding the first `title` tag within the content, and extracting its text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3b52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f71d362",
   "metadata": {},
   "source": [
    "We can get even more in-depth, and specify a path to traverse. The example below seeks the `body` tag, then the first `div` within that, and the first `div` within that!\n",
    "\n",
    "In another browser tab with the actual webpage open, try using the **Inspect Element** feature to follow the path, and confirm this is pulling the right information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b45d2eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content.body.div.div.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5850a56",
   "metadata": {},
   "source": [
    "### 1.3 CSS Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d80af90",
   "metadata": {},
   "source": [
    "All elements of a HTML document can be assigned a **class** (multiple elements can share a class) or an **id** (which are unique). These are **Cascading Style Sheet** references that ease formatting (for example, all elements containing `class='darktext'` might be defined as having a black text colour).\n",
    "\n",
    "Take the website's header as an example. This is all contained with a div called _primaryNavigation_, which we can focus on using the `find()` function. By then narrowing it down using `header > div`, we can find a few **<a\\>** tags (which are links). Let's extract the `class` of the first link that appears:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8037a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content.find('div', 'primaryNavigation').header.div.a['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606d3b09",
   "metadata": {},
   "source": [
    "From there, we can tell it to jump to the next **<a\\>** tag using `findNext()`. Within this element is an **img**, so we'll extract the class from that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347dcd67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(content.find('div', 'primaryNavigation').header.div.a.findNext('a').img['class'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261a0ded",
   "metadata": {},
   "source": [
    "The above examples are useful, but only allow us to find single elements. The `find_all()` function captures _all_ occurrences of a HTML element, by tag, and optionally by class or ID. Header text elements are defined in HTML as **h1**, **h2**, etc. Finding the text from all occurrences of `h2` tags with a specific class, for example, nicely recaps the page structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccd5088",
   "metadata": {},
   "outputs": [],
   "source": [
    "for heading in content.find_all('h2', class_='b-accordion__title'):\n",
    "    print(heading.text.strip())\n",
    "\n",
    "# alternative one line solution:\n",
    "# [x.text.strip() for x in content.find_all('h2', class_='b-accordion__title')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622e3119",
   "metadata": {},
   "source": [
    "**Task: Find the text and hyperlinks of all USYD social media platforms listed in the page footer.**\n",
    "\n",
    "The footer section of all pages on USYD's website contains a few small icons on the left, each linking to a social media account run by the university. Use \"Inspect Element\" on the webpage to find the class of the `div` that contains this information, and within this, extract the **text** _and_ **link** of each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "549acccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c1994c4",
   "metadata": {},
   "source": [
    "### 1.4 HTML tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ed4151",
   "metadata": {},
   "source": [
    "Despite being a webpage, not all useful information is stored in text fields. HTML features `table` elements, which are made up of `<tr>` rows, each consisting of `<td>` cells (or `<th>` if a header). In our example, the top of each UoS page contains an overview table of academic details. We can first locate it by it's **class**, in this case a rather lengthy _unit-details-rules__wrapper_, and explore its structure:\n",
    "\n",
    "**Note:** This is a good example in which to point out one of the troubles with web scraping - in that by configuring extraction from a website without an established endpoint (see web apis next week), the structure may be subject to change over time. In 2022 and 2023, the table below was labelled with an **id** (not a class) of _academicDetails_, but this was switched by the start of 2024, rendering prior code empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3827cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "details = content.find('div', class_='unit-details-rules__wrapper')\n",
    "details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63733ee7",
   "metadata": {},
   "source": [
    "Let's iterate through each row, and extract both its header (in **<th\\>**), and the corresponding cell data (in **<td\\>**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbb5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in details.find_all('tr'):\n",
    "    print(row.th.text, '=', row.td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e6f880",
   "metadata": {},
   "source": [
    "Note this produces output that was a bit messy, as some of the cells have a div _within_ it (for a question mark icon that users can hover over for more information). By default, BeautifulSoup will extract all text, at any depth, within this cell, therefore including text within the internal div. This can be avoided by using `.find(text=True, recursive=False)` rather than just `.text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc11abb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for row in details.find_all('tr'):\n",
    "    print(row.th.find(text=True, recursive=False).strip(), '=', row.td.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "119921ed",
   "metadata": {},
   "source": [
    "**Task: Extract the details of all assessments in the webpage.**\n",
    "\n",
    "1. Use InspectElement to locate the id/class of the div containing the assessment details (set this as `assessments`)\n",
    "2. Create a list of `headers`, for the column headers (<td\\>) of the table (e.g. ['Type', 'Description', ...])\n",
    "3. For each row in the table, add a dictionary of values for that row (e.g. {'Type': 'Online task', 'Description': 'Weekly Homework', ...}) to the `data` list\n",
    "\n",
    "Tip #1: The \"Outcomes assessed\" rows are not intended to be kept. Either skip these rows in your loop, or see if you can find a CSS class that would ignore these.\n",
    "\n",
    "Tip #2: When iterating through each cell of a row, beware that not all may be enclosed by a tag of the same type.\n",
    "\n",
    "Tip #3: A different approach may be needed for cells with bold text, and cells without bold text, to avoid the longer description text being brought in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdab246d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "assessments = '?' # use the find() function to locate the div containing the table\n",
    "\n",
    "headers = []  # populate this list with the headers of the table\n",
    "\n",
    "data = []\n",
    "for row in '?':  # iterate through each row of the table\n",
    "    assessment = {'Unit': 'DATA2001', 'Session': '2025-S1C-ND-CC'}  # start with a couple fields populated\n",
    "    # iterate through each cell in the row, and add it to the 'assessment' dictionary\n",
    "    data.append(assessment)  # add the dictionary of row values to our overall list 'data'\n",
    "\n",
    "pd.DataFrame(data)  # return the results as a dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a0efe4",
   "metadata": {},
   "source": [
    "## 2. Web Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b31a29",
   "metadata": {},
   "source": [
    "Web scraping can be very powerful, but especially so when a script can be established to do so over **multiple** webpages.\n",
    "\n",
    "Note the legal/ethical cautions, and best practices:\n",
    "1. Check the **robots.txt** to determine whether users are permitted to scrape pages, and at what frequency\n",
    "2. Add **intentional delays** in the code to avoid congesting servers (or getting blocked from websites!)\n",
    "3. Initially, just **practice** building your code over a single webpage or two. Only scale up to multiple pages once you are confident the code does as it is intended to!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae8d6ad",
   "metadata": {},
   "source": [
    "### 2.1 Link Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06e56a4",
   "metadata": {},
   "source": [
    "So far, we've been exploring the webpage for this year's occurrence of DATA2001. If we go back to the homepage for DATA2001, we can similarly parse the HTML content, and find links to all occurrences of the unit. Past occurrences are represented in the _archivedOutlines_ div, and current units are in the _currentOutlines_ div, so we'll pull links from them both.\n",
    "\n",
    "Note the use of the `.prettify()` function here, to make our printed output a little friendlier to read (here by reducing the depth of indentation, which would otherwise be many tabs deep):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac1e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "data2001page = requests.get(\"https://www.sydney.edu.au/units/DATA2001\").text\n",
    "data2001content = BeautifulSoup(data2001page, 'html5lib')\n",
    "oldlinks = data2001content.find('div', id='archivedOutlines').find_all('a')\n",
    "newlinks = data2001content.find('div', id='currentOutlines').find_all('a')\n",
    "links = oldlinks+newlinks\n",
    "for link in links:\n",
    "    print(link.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7957d1",
   "metadata": {},
   "source": [
    "Note the links there seem incomplete - they start with a slash, rather than specifying a full URL. This implies pages on the same web domain. Therefore, we can add the domain in, to turn these into fully qualified hyperlinks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdeaf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "for link in links:\n",
    "    URL = 'https://sydney.edu.au'+link['href']\n",
    "    print(URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443d4ca2",
   "metadata": {},
   "source": [
    "### 2.2 Link Traversal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24de29",
   "metadata": {},
   "source": [
    "**Task: Create a function that receives a URL, and returns the assessment data.**\n",
    "\n",
    "The function is set up below, for you to paste in your answer from the task in Section 1.4. Only a couple adjustments are needed:\n",
    "1. Your previous code worked with a predefined `content` variable. This function should receive the URL, retrieve its web contents, parse its HTML, and then proceed with this.\n",
    "2. Our previous row-by-row `assessment` dictionary had the unit and session hardcoded in. Try updating this to reflect this information dynamically from the URL itself.\n",
    "\n",
    "When confident your function is likely correct, test it runs correctly by uncommenting the last row of the cell below, which will test it on [2020's DATA2001](https://www.sydney.edu.au/units/DATA2001/2020-S1C-ND-CC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0078b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "\n",
    "def findAssessments(URL):\n",
    "    # retrieve the URL first, then:\n",
    "    \"\"\"\n",
    "    paste in your code from the task in Section 1.4, but adjust the initial 'assessment'\n",
    "    dictionary to actually detail the true unit code and session from the URL\n",
    "    \"\"\"\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "findAssessments('https://www.sydney.edu.au/units/DATA2001/2020-S1C-ND-CC')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29547fe",
   "metadata": {},
   "source": [
    "Once this has been achieved, we can test it by iterating over the links we located in Section 2.1.\n",
    "\n",
    "Note an explicit delay of two seconds has been added in between each request, using the `.sleep()` function from the `time` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e004e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time as t\n",
    "df = pd.DataFrame(columns=['Unit', 'Session']+headers)  # establishing a blank dataframe to be populated\n",
    "for link in links:  # for each link we found earlier\n",
    "    URL = 'https://sydney.edu.au'+link['href']  # establishing its full address\n",
    "    print(URL)  # printing it to summarise our progress\n",
    "    t.sleep(2)  # waiting for two seconds before requesting\n",
    "    df = pd.concat([df, findAssessments(URL)])  # merging the new data with our existing df\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e9ca1a",
   "metadata": {},
   "source": [
    "And there we have it! A simple, brief example of crawling and scraping to collate data summarised from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2c02b1",
   "metadata": {},
   "source": [
    "## 3. Data Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ca6ff3",
   "metadata": {},
   "source": [
    "Now that we have collated some information, let's note our storage options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eca1283",
   "metadata": {},
   "source": [
    "### 3.1 CSV Output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891a4233",
   "metadata": {},
   "source": [
    "As mentioned in previous coverage of Pandas, exporting to a CSV file is quite simple using the `.to_csv()` function. This should create a CSV in your working directory, containing the information we collated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536a2d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"assessments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570f9173",
   "metadata": {},
   "source": [
    "### 3.2 Database Ingestion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f44280",
   "metadata": {},
   "source": [
    "Of course, we can store this in our pgAdmin servers. The code below is taken directly from the Week 4 tutorial, defining helper functions to allow us to both connect to, and query, our individual databases. Note it requires the `Credentials.json` file from Week 4, so make sure that's in your current working directory! As always, it is recommended to launch pgAdmin in the background, so that you can close connections if you encounter the issue of too many."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02abefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, text\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "credentials = \"Credentials.json\"\n",
    "\n",
    "def pgconnect(credential_filepath, db_schema=\"public\"):\n",
    "    with open(credential_filepath) as f:\n",
    "        db_conn_dict = json.load(f)\n",
    "        host       = db_conn_dict['host']\n",
    "        db_user    = db_conn_dict['user']\n",
    "        db_pw      = db_conn_dict['password']\n",
    "        default_db = db_conn_dict['user']\n",
    "        try:\n",
    "            db = create_engine('postgresql+psycopg2://'+db_user+':'+db_pw+'@'+host+'/'+default_db, echo=False)\n",
    "            conn = db.connect()\n",
    "            print('Connected successfully.')\n",
    "        except Exception as e:\n",
    "            print(\"Unable to connect to the database.\")\n",
    "            print(e)\n",
    "            db, conn = None, None\n",
    "        return db,conn\n",
    "\n",
    "def query(conn, sqlcmd, args=None, df=True):\n",
    "    result = pd.DataFrame() if df else None\n",
    "    try:\n",
    "        if df:\n",
    "            result = pd.read_sql_query(sqlcmd, conn, params=args)\n",
    "        else:\n",
    "            result = conn.execute(text(sqlcmd), args).fetchall()\n",
    "            result = result[0] if len(result) == 1 else result\n",
    "    except Exception as e:\n",
    "        print(\"Error encountered: \", e, sep='\\n')\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc01bd1e",
   "metadata": {},
   "source": [
    "The cell below should inform you of a successful connection. If not, see our [Ed post](https://edstem.org/au/courses/8139/discussion/769731) for common issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03577494",
   "metadata": {},
   "outputs": [],
   "source": [
    "db, conn = pgconnect(credentials)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef3e250",
   "metadata": {},
   "source": [
    "We'll prepare the data load by creating a schema for it (_UnitsOfStudy_), setting our `search_path`, and deleting the Assessments table, if one does not already exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511dfb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.execute(text(\"\"\"\n",
    "create schema if not exists UnitsOfStudy;\n",
    "set search_path to UnitsOfStudy;\n",
    "drop table if exists Assessments;\n",
    "\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64ed2d4",
   "metadata": {},
   "source": [
    "We'll make two small adjustments to the dataframe, for later ease in pgAdmin.\n",
    "1. Changing all column names to lower case (case sensitivity issues addressed in Week 4 tutorial)\n",
    "2. Removing the '%' sign from the weight column, and interpreting it as a float (to allow numerical analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc30857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = map(str.lower, df.columns)\n",
    "df.weight = df.weight.str.rstrip('%').astype('float')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a7ced9d",
   "metadata": {},
   "source": [
    "Finally, we can push the data to the servers using Pandas' `.to_sql()` function, and try out a simple `select *` SQL statement to confirm the data has loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275163e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_sql(\"assessments\", con=conn, if_exists='append', index=False)\n",
    "query(conn, \"select * from Assessments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb910f37",
   "metadata": {},
   "source": [
    "### 3.3 Querying"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c109392e",
   "metadata": {},
   "source": [
    "**Task: Develop an SQL query that reports the first session, last session, and average weight of each assessment type.**\n",
    "\n",
    "Order the resulting table primarily by first session, and secondarily by last session.\n",
    "\n",
    "Optional extension: try and report the first and last _year_ rather than session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e89107",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "sql = \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "query(conn, sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3308f4fa",
   "metadata": {},
   "source": [
    "Well done on making it through! A fairly in-depth example of how web scraping can be used to tackle semi-structured data, completed by database ingestion and a simple query example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4639e0",
   "metadata": {},
   "source": [
    "## 4. Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452e159",
   "metadata": {},
   "source": [
    "The fun (but way overboard) **OPTIONAL** extra task, for students in either stream.\n",
    "\n",
    "For the last couple of years, OLEs have been a requirement of all degrees at USYD. What if we could extract assessment information **for all OLEs at the university**, thereby enabling students to narrow down those that are most appealing to them? Perhaps one student is interested in finding the OLE with the _least_ assessments, but including at least one presentation, for example. Perhaps another seeks OLEs with a group work element above a weighting of 20%. The possibilities are plentiful."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9e2ae3",
   "metadata": {},
   "source": [
    "**OPTIONAL Task: Extract the list of all OLE UoS codes/titles.**\n",
    "\n",
    "Run the below cell to request one of the 3 OLE pages (this one describes units starting with the letter A-D), then (again using Inspect Element), extract the list of all OLE titles (e.g. 'OLET1622 Numbers and Numerics') available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57da285f",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLEpage = requests.get(\"https://www.sydney.edu.au/handbooks/interdisciplinary_studies/open_learning_environment/open_learning_environment_ad_table.html\").text\n",
    "OLEcontent = BeautifulSoup(OLEpage, 'html5lib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a347654d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TO DO\n",
    "\n",
    "uoslist = '?'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c2791",
   "metadata": {},
   "source": [
    "**OPTIONAL Task: For each OLE found, extract all assessments for their most recent outline.**\n",
    "\n",
    "Iterate through the list of units extracted above, and visit each UoS page. Within this UoS page, find the latest outline link that appears (i.e. within _currentOutlines_ if possible, but _archivedOutlines_ as necessary), and apply the same `findAssessments()` function we developed earlier, again ensuring to leave an intentional delay between visiting web pages.\n",
    "\n",
    "A `progress()` helper function is included below, so that the time taken for the cell to run is reported. It is also recommended to print the unit code as each next one is reached, so that your progress can be monitored.\n",
    "\n",
    "A limit on your list of OLEs has also been added by default, so that only the first three pages are processed. Only remove this once you are confident your code will run smoothly on the remaining pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b362c68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### TO DO\n",
    "\n",
    "# helper function to report how long the cell took to run\n",
    "def progress(t0):\n",
    "    print('Completed in ' + str(round((t.time()-t0)/60, 1)) + ' minutes.')\n",
    "t0 = t.time()\n",
    "\n",
    "OLEdf = pd.DataFrame(columns=['Unit', 'Session']+headers)  # establishing a blank dataframe to be populated\n",
    "for i, uos in enumerate(uoslist[:3]):  # purposefully limiting to just the first few for now\n",
    "    print(f'({i}/{len(uoslist)})')  # printing what element in the list we're up to\n",
    "    uoscode = '?'  # locate the UoS code\n",
    "    t.sleep(2)  # wait two seconds before requesting anything\n",
    "    # request the URL for this unit and locate the first link in the unit outlines table, if it exists\n",
    "    # go to the first link in this table, and use the findAssessments() function to extract its assessment info\n",
    "    # final line should be something like: OLEdf = pd.concat([OLEdf, findAssessments(URL)])\n",
    "\n",
    "progress(t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1e67a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OLEdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ade3eb",
   "metadata": {},
   "source": [
    "From there, you are welcome to ingest this in your database server (see Step 3 instructions), and begin querying it at will, to begin discovering your \"ideal\" OLE. Any findings from this are gladly welcomed, out of general interest and the benefit of your peers!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb04919",
   "metadata": {},
   "source": [
    "#### Complete!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01763eb",
   "metadata": {},
   "source": [
    "Next week we'll delve a little further into what it means to interact with semi-structured data sources.\n",
    "\n",
    "In the meantime, have a great week!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
